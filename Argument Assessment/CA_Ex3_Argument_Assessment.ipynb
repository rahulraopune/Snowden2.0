{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA_Ex3_Argument_Assessment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdOjaHuQJ1CM"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_5resnXCiG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02114de0-9d66-4c66-db74-9e81ccc8d803"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import ssl\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Ygp_4d7_AX"
      },
      "source": [
        "# Get all negative words and Parts of speech vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADMoBN-5ROn"
      },
      "source": [
        "def fetch_all_neg_words():\n",
        "    list_neg_words = []\n",
        "    with open(\"neg_key_words.txt\",\"r\") as file:\n",
        "        for line in file:\n",
        "            list_neg_words.append(line.strip())\n",
        "    return list_neg_words"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTCobYI4uMc3"
      },
      "source": [
        "def get_number_of_neg_words(doc):\n",
        "    list_neg_words = fetch_all_neg_words()\n",
        "    sum = 0\n",
        "    for token in doc:\n",
        "        if str(token) in list_neg_words:\n",
        "            sum += 1\n",
        "    return sum"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH5lEGnOEUyK"
      },
      "source": [
        "def get_pos_vector(text):\n",
        "    \"\"\"Get the Parts-of-Speech feature vectors\"\"\"\n",
        "    pos_feature_dict = {'ADJ': 0, 'SPACE': 0, 'ADV': 0, 'INTJ': 0, 'SYM': 0, 'VERB': 0, 'SCONJ': 0, 'PART': 0, 'X': 0,\n",
        "                        'PUNCT': 0, 'AUX': 0, 'ADP': 0, 'NUM': 0, 'PRON': 0, 'NOUN': 0, 'DET': 0, 'CCONJ': 0,\n",
        "                        'PROPN': 0}\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        pos = token.pos_\n",
        "        if pos in pos_feature_dict:\n",
        "            pos_feature_dict[pos] += 1\n",
        "        else:\n",
        "            pos_feature_dict[pos] = 1\n",
        "    values_list = []\n",
        "    for k in list(pos_feature_dict.keys()):\n",
        "        values_list.append(pos_feature_dict[k])\n",
        "    return values_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKGspMK8EoF"
      },
      "source": [
        "# Split data into features and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w0xdxR35xfE"
      },
      "source": [
        "#Used to read training data\n",
        "def read_data(filename):\n",
        "    train_df = pd.read_json(filename)\n",
        "    y_train = train_df['label']\n",
        "    main_list = []\n",
        "    for i in range(0, len(train_df)):\n",
        "        id = train_df['id'][i]\n",
        "        size_of_posts_per_thread = len(train_df['preceding_posts'][i])\n",
        "        post_text = []\n",
        "        sum_contro = 0\n",
        "        sum_ups = 0\n",
        "        sum_viol = 0\n",
        "        for j in range(0, size_of_posts_per_thread): \n",
        "            text_body = train_df['preceding_posts'][i][j]['body']\n",
        "            sum_contro += train_df['preceding_posts'][i][j]['controversiality']\n",
        "            sum_ups += train_df['preceding_posts'][i][j]['ups']\n",
        "            sum_viol += train_df['preceding_posts'][i][j]['violated_rule']\n",
        "            post_text.append(text_body)\n",
        "        body_text = ' '.join(post_text)\n",
        "        doc = nlp(body_text)\n",
        "        negative_keywords_count = get_number_of_neg_words(doc)\n",
        "        main_list.append({\n",
        "            \"id\":id,\n",
        "            \"body\":body_text, \n",
        "            \"controversiality\":sum_contro, \n",
        "            \"ups\":sum_ups, \n",
        "            \"violated_rule\":sum_viol,\n",
        "            \"negative_keywords_count\":negative_keywords_count,\n",
        "            \"pos_vector\": get_pos_vector(body_text),\n",
        "            \"vector\":doc.vector\n",
        "        })\n",
        "    X_df = pd.DataFrame(main_list)\n",
        "    return X_df, y_train"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYVbA3xj7xVF"
      },
      "source": [
        "# Read Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gdWB1Lo57Zq"
      },
      "source": [
        "x_train, y_train = read_data(\"train-data-prepared.json\")\n",
        "x_val, y_val = read_data(\"val-data-prepared.json\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4iVTTNvQqK"
      },
      "source": [
        "# Generate dataframes for features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4CUO4qKs-W7"
      },
      "source": [
        "def generate_X(X_df):\n",
        "    X = []\n",
        "    for i in range(0, len(X_df)):\n",
        "        controversiality = X_df[\"controversiality\"][i]\n",
        "        ups = X_df[\"ups\"][i].item()\n",
        "        violated_rule = X_df[\"violated_rule\"][i].item()\n",
        "        negative_keywords_count = X_df[\"negative_keywords_count\"][i].item()\n",
        "        pos_vector = X_df[\"pos_vector\"][i]\n",
        "        vector = X_df[\"vector\"][i].tolist()\n",
        "        vector.append(controversiality)\n",
        "        vector.append(ups)\n",
        "        vector.append(violated_rule)\n",
        "        vector.append(negative_keywords_count)\n",
        "        vector.extend(pos_vector)\n",
        "        X.append(vector)\n",
        "    return np.array(X)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltms0E5ivWTj"
      },
      "source": [
        "X_train = generate_X(x_train)\n",
        "X_test = generate_X(x_val)\n",
        "\n",
        "y = y_train.to_numpy().T\n",
        "y_test = y_val.to_numpy().T"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlLyoFkYanLF"
      },
      "source": [
        "# Train model and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Fw4uve6A3U",
        "outputId": "adc94fba-48d6-4f4d-9547-1a66c1274132"
      },
      "source": [
        "estimator = svm.SVC(kernel='rbf', C=10000)\n",
        "y_pred = estimator.fit(X_train,y).predict(X_test)\n",
        "f1_score(y_pred=y_pred, y_true=y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6789667896678968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIRO9NryYNBe"
      },
      "source": [
        "# Writing predictions to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzTzBUE3YL9_"
      },
      "source": [
        "def write_file(predictions, id_df):\n",
        "    \"\"\"Write the predictions to JSON file\"\"\"\n",
        "    result = pd.concat([id_df, pd.DataFrame(predictions)], axis=1)\n",
        "    result.columns = ['id', 'label']\n",
        "    result.set_index('id')['label'].to_json(r'output.json')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md-8L4y6ZFQu"
      },
      "source": [
        "write_file(y_pred, x_val['id'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuSpQVeJ8LLP"
      },
      "source": [
        "# Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPdS5HyM6kQK"
      },
      "source": [
        "# def _cross_validate(estimator, X, y, X_test, y_test):\n",
        "#     parameter_space_svm = {\n",
        "#         'C':[10**i for i in range(0,8)]\n",
        "#     }\n",
        "#     from sklearn.model_selection import GridSearchCV\n",
        "#     cv = KFold(n_splits=5)\n",
        "#     clf = GridSearchCV(estimator, parameter_space_svm, scoring='f1_macro', cv=cv)\n",
        "#     clf.fit(X,y)\n",
        "        \n",
        "#     print(\"Best parameters set found on development set:\")\n",
        "#     print(clf.best_params_)\n",
        "#     y_pred = clf.predict(X_test)\n",
        "#     return f1_score(y_pred=y_pred, y_true=y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOzK_6CsvzS3"
      },
      "source": [
        "#estimator = svm.SVC(kernel='rbf')\n",
        "#print(_cross_validate(estimator, X=X_train, y=y, X_test=X_test, y_test=y_test))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FauAztDRP4nE"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}