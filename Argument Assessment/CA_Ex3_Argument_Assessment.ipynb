{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA_Ex3_Argument_Assessment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdOjaHuQJ1CM"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_5resnXCiG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c53cb7-eb6a-4245-863d-a7796a0ffa95"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import ssl\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBKhuKdi76tq"
      },
      "source": [
        "# Cleanup data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J05S40Kt5EmV"
      },
      "source": [
        "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
        "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "            \n",
        "    ## Tokenize (convert from string to list)\n",
        "    lst_text = text.split()\n",
        "    ## remove Stopwords\n",
        "    if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "                \n",
        "    ## Stemming (remove -ing, -ly, ...)\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "                \n",
        "    ## Lemmatisation (convert the word into root word)\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "            \n",
        "    ## back to string from list\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Ygp_4d7_AX"
      },
      "source": [
        "# Get all negative words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADMoBN-5ROn"
      },
      "source": [
        "def fetch_all_neg_words():\n",
        "    list_neg_words = []\n",
        "    with open(\"neg_key_words.txt\",\"r\") as file:\n",
        "        for line in file:\n",
        "            list_neg_words.append(line.strip())\n",
        "    return list_neg_words"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTCobYI4uMc3"
      },
      "source": [
        "def get_number_of_neg_words(doc):\n",
        "    list_neg_words = fetch_all_neg_words()\n",
        "    sum = 0\n",
        "    for token in doc:\n",
        "        if str(token) in list_neg_words:\n",
        "            sum += 1\n",
        "    return sum"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKGspMK8EoF"
      },
      "source": [
        "# Split data into features and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w0xdxR35xfE"
      },
      "source": [
        "#Used to read training data\n",
        "def read_data(filename):\n",
        "    train_df = pd.read_json(filename)\n",
        "    y_train = train_df['label']\n",
        "    body_train = []\n",
        "    contro_list = []\n",
        "    ups_list = []\n",
        "    viol_list = []\n",
        "    main_list = []\n",
        "    for i in range(0, len(train_df)):\n",
        "        id = train_df['id'][i]\n",
        "        size_of_posts_per_thread = len(train_df['preceding_posts'][i])\n",
        "        post_text = []\n",
        "        sum_contro = 0\n",
        "        sum_ups = 0\n",
        "        sum_viol = 0\n",
        "        for j in range(0, size_of_posts_per_thread): \n",
        "            text_body = train_df['preceding_posts'][i][j]['body']\n",
        "            sum_contro += train_df['preceding_posts'][i][j]['controversiality']\n",
        "            sum_ups += train_df['preceding_posts'][i][j]['ups']\n",
        "            sum_viol += train_df['preceding_posts'][i][j]['violated_rule']\n",
        "            post_text.append(text_body)\n",
        "        body_text = ' '.join(post_text)\n",
        "        preprocessed_body_text = utils_preprocess_text(body_text,lst_stopwords=lst_stopwords)\n",
        "        # doc = nlp(preprocessed_body_text) # pre-process\n",
        "        doc = nlp(body_text) # no pre-processing\n",
        "        negative_keywords_count = get_number_of_neg_words(doc)\n",
        "        main_list.append({\n",
        "            \"id\":id,\n",
        "            \"body\":preprocessed_body_text, \n",
        "            \"controversiality\":sum_contro, \n",
        "            \"ups\":sum_ups, \n",
        "            \"violated_rule\":sum_viol,\n",
        "            \"negative_keywords_count\":negative_keywords_count,\n",
        "            \"vector\":doc.vector\n",
        "        })\n",
        "    X_df = pd.DataFrame(main_list)\n",
        "    return X_df, y_train"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYVbA3xj7xVF"
      },
      "source": [
        "# Read Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gdWB1Lo57Zq"
      },
      "source": [
        "x_train, y_train = read_data(\"train-data-prepared.json\")\n",
        "x_val, y_val = read_data(\"val-data-prepared.json\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4iVTTNvQqK"
      },
      "source": [
        "# Generate dataframes for features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4CUO4qKs-W7"
      },
      "source": [
        "def generate_X(X_df):\n",
        "    X = []\n",
        "    for i in range(0, len(X_df)):\n",
        "        controversiality = X_df[\"controversiality\"][i]\n",
        "        ups = X_df[\"ups\"][i].item()\n",
        "        violated_rule = X_df[\"violated_rule\"][i].item()\n",
        "        negative_keywords_count = X_df[\"negative_keywords_count\"][i].item()\n",
        "        vector = X_df[\"vector\"][i].tolist()\n",
        "        vector.append(controversiality)\n",
        "        vector.append(ups)\n",
        "        vector.append(violated_rule)\n",
        "        vector.append(negative_keywords_count)\n",
        "        X.append(vector)\n",
        "    return np.array(X)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltms0E5ivWTj"
      },
      "source": [
        "X_train = generate_X(x_train)\n",
        "X_test = generate_X(x_val)\n",
        "\n",
        "y = y_train.to_numpy().T\n",
        "y_test = y_val.to_numpy().T"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlLyoFkYanLF"
      },
      "source": [
        "# Train model and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Fw4uve6A3U",
        "outputId": "21d4fe1b-3de3-4b8f-cf3e-370a0384879e"
      },
      "source": [
        "estimator = svm.SVC(kernel='rbf', C=10000)\n",
        "y_pred = estimator.fit(X_train,y).predict(X_test)\n",
        "f1_score(y_pred=y_pred, y_true=y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6980392156862745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIRO9NryYNBe"
      },
      "source": [
        "# Writing predictions to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzTzBUE3YL9_"
      },
      "source": [
        "def write_file(predictions, id_df):\n",
        "    \"\"\"Write the predictions to JSON file\"\"\"\n",
        "    result = pd.concat([id_df, pd.DataFrame(predictions)], axis=1)\n",
        "    result.columns = ['id', 'label']\n",
        "    result.set_index('id')['label'].to_json(r'output.json')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md-8L4y6ZFQu"
      },
      "source": [
        "write_file(y_pred, x_val['id'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuSpQVeJ8LLP"
      },
      "source": [
        "# Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPdS5HyM6kQK"
      },
      "source": [
        "def _cross_validate(estimator, X, y, X_test, y_test):\n",
        "    parameter_space_svm = {\n",
        "        'C':[10**i for i in range(0,8)]\n",
        "    }\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    cv = KFold(n_splits=5)\n",
        "    clf = GridSearchCV(estimator, parameter_space_svm, scoring='f1_macro', cv=cv)\n",
        "    clf.fit(X,y)\n",
        "        \n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print(clf.best_params_)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return f1_score(y_pred=y_pred, y_true=y_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOzK_6CsvzS3",
        "outputId": "03b42554-71a9-4ab7-a5b5-7b6a6b3b30fb"
      },
      "source": [
        "estimator = svm.SVC(kernel='rbf')\n",
        "print(_cross_validate(estimator, X=X_train, y=y, X_test=X_test, y_test=y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 10000}\n",
            "\n",
            "0.6980392156862745\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}